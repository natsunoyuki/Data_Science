{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch vs TensorFlow MNIST.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOhUyxj8S6Nbs4oDK3XHRJB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import tensorflow as tf\n","import time\n","\n","import torch\n","\n","# PyTorch\n","# Connect to GPU is possible.\n","if torch.cuda.is_available():\n","    device_name = torch.device(\"cuda\")\n","else:\n","    device_name = torch.device('cpu')\n","\n","print(\"Using {}.\".format(device_name))\n","\n","# TensorFlow\n","try:\n","    DEVICE_NAME = tf.test.gpu_device_name()\n","    print(\"Found GPU at: {}\".format(DEVICE_NAME))\n","except:\n","    DEVICE_NAME = \"/device:CPU:0\"\n","    print(\"ERROR: Not connected to a GPU runtime.\")"],"metadata":{"id":"vwvNHitvU9rG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653806180310,"user_tz":-540,"elapsed":10409,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"40439b72-13d5-4fe4-b79e-a6000149da07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda.\n","Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["# Download the data using TensorFlow\n","\n","def download_mnist_data(channels = None, categorize = False):\n","    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","    # Normalize the pixel values from [0, 255] to [0, 1].\n","    X_train = X_train.astype(float) / 255\n","    X_test = X_test.astype(float) / 255\n","\n","    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 10000, random_state = 42)\n","    print(\"Loaded {} train, {} validation and {} test images.\".format(len(X_train), len(X_valid), len(X_test)))\n","\n","    if channels == \"NHWC\":\n","        X_train = X_train.reshape([*X_train.shape, 1])\n","        X_valid = X_valid.reshape([*X_valid.shape, 1])\n","        X_test = X_test.reshape([*X_test.shape, 1])\n","    elif channels == \"NCHW\":\n","        X_train = X_train.reshape([X_train.shape[0], 1, X_train.shape[1], X_train.shape[2]])\n","        X_valid = X_valid.reshape([X_valid.shape[0], 1, X_valid.shape[1], X_valid.shape[2]])\n","        X_test = X_test.reshape([X_test.shape[0], 1, X_test.shape[1], X_test.shape[2]])\n","\n","    if categorize == True:\n","        y_train = tf.keras.utils.to_categorical(y_train)\n","        y_valid = tf.keras.utils.to_categorical(y_valid)\n","        y_test = tf.keras.utils.to_categorical(y_test)\n","\n","    return X_train, y_train, X_valid, y_valid, X_test, y_test"],"metadata":{"id":"EzN8BK5jWi6E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PyTorch"],"metadata":{"id":"PhxUSYrzCd7L"}},{"cell_type":"code","source":["def make_torch_dataloader(X, y, batch_size = 20, shuffle = False):\n","    X = torch.from_numpy(X).to(torch.float32)\n","    y = torch.from_numpy(y).to(torch.int64)\n","    ds = torch.utils.data.TensorDataset(X, y)\n","    return torch.utils.data.DataLoader(ds, batch_size = batch_size, shuffle = shuffle)\n","\n","def make_torch_dataloaders(X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size = 20):\n","    train_ds = make_torch_dataloader(X_train, y_train, batch_size = batch_size, shuffle = True)\n","    valid_ds = make_torch_dataloader(X_valid, y_valid, batch_size = batch_size)\n","    test_ds = make_torch_dataloader(X_test, y_test, batch_size = batch_size)\n","\n","    return train_ds, valid_ds, test_ds\n","\n","batch_size = 20\n","X_train, y_train, X_valid, y_valid, X_test, y_test = download_mnist_data(channels = \"NCHW\", categorize = False)\n","train_dl, valid_dl, test_dl = make_torch_dataloaders(X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size = batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5mPwdbMVmAlE","executionInfo":{"status":"ok","timestamp":1653806181665,"user_tz":-540,"elapsed":1362,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"26aa62d9-a917-4d44-ab37-da7a2feb2e21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 50000 train, 10000 validation and 10000 test images.\n"]}]},{"cell_type":"code","source":["class PyTorchMnistCNNClassifier(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # Tensor shape: [N, 1, 28, 28]\n","        self.conv1 = torch.nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size = 3, padding = 1)\n","        self.relu1 = torch.nn.ReLU()\n","        self.pool1 = torch.nn.MaxPool2d(kernel_size = 2)\n","        # Tensor shape: [N, 16, 14, 14]\n","        self.conv2 = torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, padding = 1)\n","        self.relu2 = torch.nn.ReLU()\n","        self.pool2 = torch.nn.MaxPool2d(kernel_size = 2)\n","        # Tensor shape: [N, 32, 7, 7]\n","        self.flatten = torch.nn.Flatten()\n","        # Tensor shape: [N, 1568]\n","        self.linear1 = torch.nn.Linear(in_features = 1568, out_features = 1024)\n","        self.relu3 = torch.nn.ReLU()\n","        self.dropout = torch.nn.Dropout(p = 0.5)\n","        # Tensor shape: [N, 1024]\n","        self.linear2 = torch.nn.Linear(in_features = 1024, out_features = 10)\n","        # Tensor shape: [N, 10]\n","\n","    def forward(self, x, training = False):\n","        x = self.conv1(x)\n","        x = self.relu1(x)\n","        x = self.pool1(x)\n","\n","        x = self.conv2(x)\n","        x = self.relu2(x)\n","        x = self.pool2(x)\n","\n","        x = self.flatten(x)\n","        x = self.linear1(x)\n","        x = self.relu3(x)\n","        x = self.dropout(x)\n","        x = self.linear2(x)\n","\n","        return x"],"metadata":{"id":"glhrGzDQLuOy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def xy_to_device(x, y, device = \"cpu\"):\n","    return x.to(device), y.to(device)\n","\n","def forward_pass(x, y, model, loss_fn):\n","    y_pred = model(x)\n","    loss = loss_fn(y_pred, y)\n","    return y_pred, loss\n","\n","def back_propagation(loss, optimizer):\n","    # 0. Reset the gradients.\n","    optimizer.zero_grad()\n","    # 1. Calculate gradients from loss function.\n","    loss.backward()\n","    # 2. Update model weights with gradients.\n","    optimizer.step()\n","\n","def train_loop(model, loss_fn, optimizer, train_dl, metric_fn = None, device = \"cpu\"):\n","    loss_hist = 0\n","    if metric_fn is not None:\n","        metric_hist = 0\n","\n","    # Set model to training mode.\n","    model.train()\n","    # Move model to device (CPU or GPU).\n","    model.to(device) \n","    for x, y in train_dl:\n","        # 1. Move data to device (CPU or GPU).\n","        x, y = xy_to_device(x, y, device)\n","        # 2. Model forward pass.\n","        pred, loss = forward_pass(x, y, model, loss_fn)\n","        # 3. Model back propagation.\n","        back_propagation(loss, optimizer)\n","        # 4. Update the metrics. Divide by the batch size to get the mean.\n","        loss_hist = loss_hist + loss.item()\n","        if metric_fn is not None:\n","            metric_hist = metric_hist + metric_fn(pred, y)\n","\n","    # Divide by the number of batches to get the epoch mean.\n","    loss_hist = loss_hist / len(train_dl)\n","\n","    if metric_fn is not None:\n","        # Divide by the number of batches to get the epoch mean.\n","        metric_hist = metric_hist / len(train_dl)\n","        return loss_hist, metric_hist\n","    else:\n","        return loss_hist\n","\n","def evaluation_loop(model, loss_fn, valid_dl, metric_fn = None, device = \"cpu\"):\n","    loss_hist = 0\n","    if metric_fn is not None:\n","        metric_hist = 0\n","\n","    # Set model to evaluation mode.\n","    model.eval()\n","    model.to(device)\n","    # Switch gradients off to conserve resources.\n","    with torch.no_grad():\n","        for x, y in valid_dl:\n","            # 1. Move data to device (CPU or GPU).\n","            x, y = xy_to_device(x, y, device)\n","            # 2. Model forward pass.\n","            pred, loss = forward_pass(x, y, model, loss_fn)\n","            # 3. Update the metrics.\n","            loss_hist = loss_hist + loss.item()\n","            if metric_fn is not None:\n","                metric_hist = metric_hist + metric_fn(pred, y) \n","\n","    loss_hist = loss_hist / len(valid_dl)\n","\n","    if metric_fn is not None:\n","        metric_hist = metric_hist / len(valid_dl)\n","        return loss_hist, metric_hist\n","    else:\n","        return loss_hist\n","\n","def train(model, loss_fn, optimizer, num_epochs, train_dl, valid_dl, metric_fn, verbose = False, device = \"cpu\"):\n","    train_loss = [] \n","    train_acc = []\n","    valid_loss = [] \n","    valid_acc = []\n","\n","    # Repeat the training for the specified number of epochs.\n","    for i in range(num_epochs):\n","        # Training loop.\n","        loss_hist, acc_hist = train_loop(model, loss_fn, optimizer, train_dl, metric_fn, device = device)\n","        train_loss.append(loss_hist)\n","        train_acc.append(acc_hist)\n","\n","        # Evaluation loop.\n","        loss_hist, acc_hist = evaluation_loop(model, loss_fn, valid_dl, metric_fn, device = device)\n","        valid_loss.append(loss_hist)\n","        valid_acc.append(acc_hist)\n","\n","        if verbose == True:\n","            epoch_str = \"Epoch {:3d}: \".format(i+1)\n","            loss_str = \"loss: {:.3f} \".format(train_loss[i])\n","            acc_str = \"accuracy: {:.3f}, \".format(train_acc[i])\n","            val_loss_str = \"val_loss: {:.3f} \".format(valid_loss[i])\n","            val_acc_str = \"val_accuracy: {:.3f}.\".format(valid_acc[i])\n","            print(epoch_str + loss_str + acc_str + val_loss_str + val_acc_str)\n","            \"\"\"\n","            print(\"Epoch {:3d}: loss: {:.3f} accuracy: {:.3f}, val_loss: {:.3f} val_accuracy: {:.3f}.\".format(1+i, \n","                                                                                                    train_loss[i],\n","                                                                                                     train_acc[i],\n","                                                                                                    valid_loss[i],\n","                                                                                                     valid_acc[i]))\"\"\"\n","    return train_loss, train_acc, valid_loss, valid_acc"],"metadata":{"id":"RjV80yDoMpWI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the model, loss function, optimizer and metric objects.\n","model = PyTorchMnistCNNClassifier()\n","loss_fn = torch.nn.CrossEntropyLoss(reduction = \"mean\")\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","\n","def accuracy(y_pred, y_true):\n","    correct = (torch.argmax(y_pred, dim = 1) == y_true).float()\n","    return correct.mean()\n","\n","# Train the model.\n","start_time = time.time()\n","torch.manual_seed(42)\n","num_epochs = 10\n","hist = train(model, loss_fn, optimizer, num_epochs, train_dl, valid_dl, accuracy, True, device_name)\n","print(\"Time elapsed: {:.2f}s.\".format(time.time() - start_time))"],"metadata":{"id":"XKn6i_iWPdDI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653806272161,"user_tz":-540,"elapsed":90254,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"8ca56132-ccd5-455b-9021-ac6ef04af879"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch   1: loss: 0.181 accuracy: 0.945, val_loss: 0.059 val_accuracy: 0.983.\n","Epoch   2: loss: 0.063 accuracy: 0.981, val_loss: 0.044 val_accuracy: 0.986.\n","Epoch   3: loss: 0.046 accuracy: 0.986, val_loss: 0.047 val_accuracy: 0.986.\n","Epoch   4: loss: 0.034 accuracy: 0.989, val_loss: 0.038 val_accuracy: 0.990.\n","Epoch   5: loss: 0.028 accuracy: 0.991, val_loss: 0.037 val_accuracy: 0.989.\n","Epoch   6: loss: 0.023 accuracy: 0.993, val_loss: 0.038 val_accuracy: 0.990.\n","Epoch   7: loss: 0.019 accuracy: 0.994, val_loss: 0.035 val_accuracy: 0.991.\n","Epoch   8: loss: 0.016 accuracy: 0.995, val_loss: 0.046 val_accuracy: 0.991.\n","Epoch   9: loss: 0.016 accuracy: 0.995, val_loss: 0.040 val_accuracy: 0.991.\n","Epoch  10: loss: 0.012 accuracy: 0.997, val_loss: 0.045 val_accuracy: 0.990.\n","Time elapsed: 89.87s.\n"]}]},{"cell_type":"code","source":["test_loss, test_acc = evaluation_loop(model, loss_fn, test_dl, accuracy, device_name)\n","\n","print(\"Test set loss: {:.3f}, test set accuracy: {:.3f}.\".format(test_loss, test_acc))"],"metadata":{"id":"I7Fx0DysPiqV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653806272522,"user_tz":-540,"elapsed":391,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"dbe90d18-e0ae-4df0-f2db-1abdc10aa6a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test set loss: 0.031, test set accuracy: 0.992.\n"]}]},{"cell_type":"code","source":["# Predictions\n","\n","def predict(model, dl, device = \"cpu\"):\n","    model.to(device)\n","    pred = torch.Tensor([])\n","    for b in dl:\n","        x_bat = b[0]\n","        y_bat = b[1]\n","        x_bat = x_bat.to(device)\n","        y_bat = y_bat.to(device)\n","        pred = torch.cat([pred, model(x_bat).argmax(axis = 1).to(torch.int64)])\n","\n","    return pred\n","\n","pred = predict(model, test_dl)"],"metadata":{"id":"s5l9U52ec3yz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(y_test, pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q5gT6aqvdxEc","executionInfo":{"status":"ok","timestamp":1653806300384,"user_tz":-540,"elapsed":327,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"62f545fd-d393-4452-fe50-e822c4ac3eec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.99      1.00      0.99       980\n","           1       1.00      1.00      1.00      1135\n","           2       0.99      0.99      0.99      1032\n","           3       0.99      1.00      0.99      1010\n","           4       1.00      0.99      0.99       982\n","           5       0.99      0.99      0.99       892\n","           6       1.00      0.99      0.99       958\n","           7       0.99      0.99      0.99      1028\n","           8       0.99      0.99      0.99       974\n","           9       0.99      0.99      0.99      1009\n","\n","    accuracy                           0.99     10000\n","   macro avg       0.99      0.99      0.99     10000\n","weighted avg       0.99      0.99      0.99     10000\n","\n"]}]},{"cell_type":"markdown","source":["# TensorFlow"],"metadata":{"id":"emB8AKwXCf-Z"}},{"cell_type":"code","source":["def make_tf_dataset(X, y, batch_size = 20, shuffle = False):\n","    X = tf.data.Dataset.from_tensor_slices(X)\n","    y = tf.data.Dataset.from_tensor_slices(y)\n","    ds = tf.data.Dataset.zip((X, y))\n","    if shuffle == True:\n","        ds = ds.shuffle(len(X))\n","    return ds.batch(batch_size)\n","\n","def make_tf_datasets(X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size = 20):\n","    train_ds = make_tf_dataset(X_train, y_train, batch_size = batch_size, shuffle = True)\n","    valid_ds = make_tf_dataset(X_valid, y_valid, batch_size = batch_size)\n","    test_ds = make_tf_dataset(X_test, y_test, batch_size = batch_size)\n","    return train_ds, valid_ds, test_ds\n","\n","batch_size = 20\n","X_train, y_train, X_valid, y_valid, X_test, y_test = download_mnist_data(channels = \"NHWC\", categorize = True)\n","train_ds, valid_ds, test_ds = make_tf_datasets(X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size = batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Adk3Mid6rs3-","executionInfo":{"status":"ok","timestamp":1653806304858,"user_tz":-540,"elapsed":1777,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"ed71fce1-b4eb-4118-c2ef-82e554d9120a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 50000 train, 10000 validation and 10000 test images.\n"]}]},{"cell_type":"code","source":["class TFMnistCNNClassifier(tf.keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        # Tensor shape: [N, 28, 28, 1]\n","        self.conv1 = tf.keras.layers.Conv2D(filters = 16, kernel_size = 3, padding = \"same\", activation = \"relu\", input_shape = (28, 28, 1))\n","        self.pool1 = tf.keras.layers.MaxPool2D([2, 2])\n","        # Tensor shape: [N, 14, 14, 16]\n","        self.conv2 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, padding = \"same\", activation = \"relu\")\n","        self.pool2 = tf.keras.layers.MaxPool2D([2, 2])\n","        # Tensor shape: [N, 7, 7, 32]\n","        self.flatten = tf.keras.layers.Flatten()\n","        # Tensor shape: [N, 1568]\n","        self.linear1 = tf.keras.layers.Dense(1024, activation = \"relu\")\n","        # Tensor shape: [N, 1024]\n","        self.dropout = tf.keras.layers.Dropout(0.5)\n","        self.linear2 = tf.keras.layers.Dense(10, activation = \"softmax\")\n","        # Tensor shape: [N, 10]\n","\n","    def call(self, x, training = False):\n","        x = self.conv1(x)\n","        x = self.pool1(x)\n","\n","        x = self.conv2(x)\n","        x = self.pool2(x)\n","\n","        x = self.flatten(x)\n","        x = self.linear1(x)\n","        x = self.dropout(x)\n","        x = self.linear2(x)\n","        return x\n","\n","def make_tf_model(input_shape = [28, 28, 1]):\n","    # Create model architecture.\n","    model = TFMnistCNNClassifier()\n","    # Set model inputs.\n","    inputs = tf.keras.layers.Input(input_shape)\n","    # Set model outputs.\n","    outputs = model(inputs)\n","    # Create model for training.\n","    return tf.keras.Model(inputs = inputs, outputs = outputs)\n","\n","def make_tf_model_on_device(device = \"/device:CPU:0\"):\n","    with tf.device(device):\n","        tf_model = make_tf_model()\n","        tf_model.compile(loss = tf.keras.losses.CategoricalCrossentropy(), \n","                        optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n","                        metrics = tf.keras.metrics.CategoricalAccuracy())\n","    return tf_model"],"metadata":{"id":"HJKmtF_bENOF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_model = make_tf_model_on_device(device = DEVICE_NAME)\n","  \n","history = tf_model.fit(train_ds, batch_size = batch_size, epochs = num_epochs, validation_data = valid_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_YRTum64JkJG","executionInfo":{"status":"ok","timestamp":1653806464451,"user_tz":-540,"elapsed":158404,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"cb22527f-e66d-4c94-8e57-f119f917cc8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","2500/2500 [==============================] - 18s 6ms/step - loss: 0.1479 - categorical_accuracy: 0.9534 - val_loss: 0.0478 - val_categorical_accuracy: 0.9855\n","Epoch 2/10\n","2500/2500 [==============================] - 11s 4ms/step - loss: 0.0536 - categorical_accuracy: 0.9827 - val_loss: 0.0387 - val_categorical_accuracy: 0.9881\n","Epoch 3/10\n","2500/2500 [==============================] - 11s 4ms/step - loss: 0.0387 - categorical_accuracy: 0.9872 - val_loss: 0.0335 - val_categorical_accuracy: 0.9904\n","Epoch 4/10\n","2500/2500 [==============================] - 11s 4ms/step - loss: 0.0314 - categorical_accuracy: 0.9901 - val_loss: 0.0399 - val_categorical_accuracy: 0.9885\n","Epoch 5/10\n","2500/2500 [==============================] - 11s 4ms/step - loss: 0.0253 - categorical_accuracy: 0.9916 - val_loss: 0.0339 - val_categorical_accuracy: 0.9909\n","Epoch 6/10\n","2500/2500 [==============================] - 11s 4ms/step - loss: 0.0215 - categorical_accuracy: 0.9929 - val_loss: 0.0343 - val_categorical_accuracy: 0.9924\n","Epoch 7/10\n","2500/2500 [==============================] - 11s 4ms/step - loss: 0.0177 - categorical_accuracy: 0.9941 - val_loss: 0.0435 - val_categorical_accuracy: 0.9897\n","Epoch 8/10\n","2500/2500 [==============================] - 11s 4ms/step - loss: 0.0160 - categorical_accuracy: 0.9950 - val_loss: 0.0373 - val_categorical_accuracy: 0.9914\n","Epoch 9/10\n","2500/2500 [==============================] - 11s 4ms/step - loss: 0.0130 - categorical_accuracy: 0.9956 - val_loss: 0.0506 - val_categorical_accuracy: 0.9902\n","Epoch 10/10\n","2500/2500 [==============================] - 11s 4ms/step - loss: 0.0147 - categorical_accuracy: 0.9951 - val_loss: 0.0448 - val_categorical_accuracy: 0.9917\n"]}]},{"cell_type":"code","source":["tf_model.evaluate(test_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fzIGfWaLLnYa","executionInfo":{"status":"ok","timestamp":1653806465915,"user_tz":-540,"elapsed":1480,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"8684cd57-7a5b-43f5-f73c-ea91fc12dbfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["500/500 [==============================] - 2s 3ms/step - loss: 0.0300 - categorical_accuracy: 0.9923\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.03002397157251835, 0.9922999739646912]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Make class predictions on test_ds using argmax.\n","y_pred = tf_model.predict(test_ds).argmax(axis = 1)\n","\n","# Extract the true y-labels from test_ds. \n","# We could have simply used y_test.argmax(axis = 1) as well!\n","y_true = np.array([np.argmax(bat[1]) for bat in test_ds.unbatch()])\n","\n","# Use the predictions to make a classification report.\n","print(classification_report(y_true, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCShcUumbvlA","executionInfo":{"status":"ok","timestamp":1653806468413,"user_tz":-540,"elapsed":2503,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"fa3d926d-c2d5-4d60-e98c-74debccbe0fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00       980\n","           1       1.00      0.99      1.00      1135\n","           2       1.00      0.99      0.99      1032\n","           3       0.99      1.00      0.99      1010\n","           4       1.00      0.99      0.99       982\n","           5       0.99      0.99      0.99       892\n","           6       0.99      0.99      0.99       958\n","           7       0.99      0.99      0.99      1028\n","           8       0.98      0.99      0.99       974\n","           9       0.99      0.99      0.99      1009\n","\n","    accuracy                           0.99     10000\n","   macro avg       0.99      0.99      0.99     10000\n","weighted avg       0.99      0.99      0.99     10000\n","\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"zH-v-zwKg2eJ"},"execution_count":null,"outputs":[]}]}