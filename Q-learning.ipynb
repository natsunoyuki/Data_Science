{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMuCa1bLPN8/236S7VeZbS5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Q-Learning\n","\n","#### Rendering Gym in Colab\n","https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t"],"metadata":{"id":"iXUi2IStSDZD"}},{"cell_type":"code","source":["#!pip install gym==0.20.0\n","#!pip install torch==1.8.0\n","#!pip install numpy==1.21.2\n","#!pip install matplotlib==3.4.3\n","#!pip install gym[box2d]"],"metadata":{"id":"M0xkWFuRTl-A","executionInfo":{"status":"ok","timestamp":1681050425378,"user_tz":-540,"elapsed":677,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict, namedtuple\n","import tqdm\n","import gym\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd"],"metadata":{"id":"GDBCDefLQ4F0","executionInfo":{"status":"ok","timestamp":1681050426074,"user_tz":-540,"elapsed":29,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["print(\"gym=={}\".format(gym.__version__))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qz8qwjW6HCG3","executionInfo":{"status":"ok","timestamp":1681050426074,"user_tz":-540,"elapsed":27,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"b77196d3-1cb1-4fbd-8e23-f21c454bf843"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["gym==0.25.2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","source":["# Frozen Lake Environment\n","\n","https://www.gymlibrary.dev/environments/toy_text/frozen_lake/"],"metadata":{"id":"UxeAOVuydsE9"}},{"cell_type":"code","source":["# Create the Frozen lake environment\n","# with 4 columns and 4 rows.\n","env = gym.make('FrozenLake-v1', \n","               desc = None, \n","               map_name = \"4x4\", \n","               is_slippery = False)\n","\n","# There are 4 possible actions the agent can take:\n","# Move up, move down, move left and move right.\n","# Each action results in the agent moving to a \n","# new x-y position, thereby changing the state\n","# of the environment.\n","# As the environment is a 4x4 grid, there are\n","# 16 possible states.\n","print(\"Actions: {}, states: {}.\".format(env.action_space, \n","                                   env.observation_space))"],"metadata":{"id":"Quajql6Eo_9d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681050426075,"user_tz":-540,"elapsed":22,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"5d5ff849-84c4-4dd1-e8e2-20e82b986fc1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Actions: Discrete(4), states: Discrete(16).\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["# Reset the environment and get the reset state.\n","state = env.reset(seed = 123)\n","print(\"Reset state: {}.\".format(state))\n","\n","actions = [1, 1, 2, 2, 1, 2]\n","\n","# Take a series of actions and print the resulting state.\n","for action in actions:\n","    # Each action changes the state of the environment.\n","    next_state, reward, done, info = env.step(action)\n","    print(\"Next state: {:2d}, reward: {}, game end: {}.\".format(next_state, \n","                                                                reward, \n","                                                                done))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jPo7ROH36hMi","executionInfo":{"status":"ok","timestamp":1681050426075,"user_tz":-540,"elapsed":15,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"0ffacdce-ec71-46ce-86da-7e70eb8474e9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Reset state: 0.\n","Next state:  4, reward: 0.0, game end: False.\n","Next state:  8, reward: 0.0, game end: False.\n","Next state:  9, reward: 0.0, game end: False.\n","Next state: 10, reward: 0.0, game end: False.\n","Next state: 14, reward: 0.0, game end: False.\n","Next state: 15, reward: 1.0, game end: True.\n"]}]},{"cell_type":"code","source":["# Reset the environment and get the reset state.\n","state = env.reset(seed = 123)\n","print(\"Reset state: {}.\".format(state))\n","\n","actions = [1, 2]\n","\n","# Take a series of actions and print the resulting state.\n","for action in actions:\n","    # Each action changes the state of the environment.\n","    next_state, reward, done, info = env.step(action)\n","    print(\"Next state: {:2d}, reward: {}, game end: {}.\".format(next_state, \n","                                                                reward, \n","                                                                done))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQ6mX8ZwA-rn","executionInfo":{"status":"ok","timestamp":1681050426075,"user_tz":-540,"elapsed":9,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"9cfe5088-0085-403a-c5a5-d491d8889433"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Reset state: 0.\n","Next state:  4, reward: 0.0, game end: False.\n","Next state:  5, reward: 0.0, game end: True.\n"]}]},{"cell_type":"markdown","source":["# Q-Learning Agent\n","\n","https://github.com/rasbt/machine-learning-book/blob/main/ch19/ch19.ipynb"],"metadata":{"id":"nDBC-HPmduZI"}},{"cell_type":"code","source":["class Agent(object):\n","    def __init__(self, \n","                 env,                         # gym env.\n","                 learning_rate = 0.7,         # Q-learning learning rate.\n","                 discount_factor = np.log(2), # Q-learning discount factor.\n","                 epsilon = 1.0,               # Greedy epsilon factor.\n","                 epsilon_min = 0.01,          # Minimum epsilon value to decay to.\n","                 epsilon_decay = 0.99,        # Epsilon decay rate.\n","                 train_mode = True            # Train mode. \n","                 ):   \n","      \n","        # Q-learning hyperparameters.\n","        self.learning_rate = learning_rate     # Q-learning learning rate.\n","        self.discount_factor = discount_factor # Q-learning discount factor.\n","        self.epsilon = epsilon                 # Greedy epsilon factor.\n","        self.epsilon_min = epsilon_min         # Minimum epsilon value to decay to.\n","        self.epsilon_decay = epsilon_decay     # Epsilon decay rate.\n","\n","        # Environment hyperparameters.\n","        self.env = env          \n","        self.nA = env.action_space.n\n","\n","        # Table of Q-values.\n","        self.q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","        # If train mode is False, then epsilon will be set to 0, and the\n","        # agent will choose the action to be taken solely from the Q-table\n","        # and not use any randomness.\n","        self.set_train_mode(train_mode)\n"," \n","    def choose_action(self, state):\n","        if self.train_mode == True:\n","            action = np.argmax(self.q_table[state])\n","        else: \n","            # Exploration - randomly sample an action.\n","            # Only applicable for training.\n","            if np.random.uniform() < self.epsilon:\n","                action = env.action_space.sample()\n","            # Exploitation - choose the best (highest Q-value) action given some\n","            # current state.\n","            # For non-training, this is the only way the model\n","            # will choose the next action.\n","            else:\n","                action = np.argmax(self.q_table[state])\n","        return action\n","\n","    def set_train_mode(self, train_mode):\n","        self.train_mode = train_mode\n","\n","    def _learn(self, transition):\n","        s, a, r, next_s, done = transition\n","        q_val = self.q_table[s][a]\n","        if done:\n","            q_target = r\n","        else:\n","            q_target = r + self.discount_factor * np.max(self.q_table[next_s])\n","\n","        self.q_table[s][a] = self.q_table[s][a] + self.learning_rate * (q_target - q_val)\n","        self._adjust_epsilon()\n","\n","    def _adjust_epsilon(self):\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon = self.epsilon * self.epsilon_decay"],"metadata":{"id":"8MbAz-VoQktC","executionInfo":{"status":"ok","timestamp":1681050426075,"user_tz":-540,"elapsed":5,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def run_qlearning(agent, env, num_episodes = 50):\n","    history = {\"states\" : [], \"actions\" : [], \"n_moves\" : [], \"rewards\" : []}\n","\n","    for episode in range(num_episodes):\n","        state = env.reset(seed = 123)\n","\n","        previous_state = state\n","\n","        states = [state]\n","        actions = []\n","        step = 0\n","        while True:\n","            step = step + 1\n","            # Agent chooses action to take based on current state.\n","            action = agent.choose_action(state)\n","\n","            # Performing an action changes the environment's state.\n","            next_s, reward, done, _ = env.step(action)\n","\n","            # If the agent walks into a hole, a penalty is incurred.\n","            if done == True and reward == 0:\n","                reward = -10\n","            # If the agent finds the wrapped gift, a reward in incurred.\n","            elif done == True and reward == 1:\n","                reward = 10\n","\n","            # If the agent returns to the previous state or stays in the same\n","            # state, a penalty is incurred.\n","            if next_s == previous_state:\n","                reward = -10\n","            if next_s == state:\n","                reward = -10\n","\n","            # Q-learning.\n","            agent._learn((state, action, reward, next_s, done))\n","\n","            states.append(next_s)\n","            actions.append(action)\n","\n","            previous_state = state\n","            state = next_s\n","\n","            if done:\n","                break\n","\n","            final_reward = reward\n","\n","        history[\"states\"].append(states)\n","        history[\"actions\"].append(actions)\n","        history[\"n_moves\"].append(step)\n","        history[\"rewards\"].append(final_reward)\n","\n","    return history"],"metadata":{"id":"hJ5p1wbWQq1k","executionInfo":{"status":"ok","timestamp":1681050426512,"user_tz":-540,"elapsed":15,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Create environment and agent.\n","\n","env = gym.make('FrozenLake-v1', \n","               desc = None, \n","               map_name = \"4x4\", \n","               is_slippery = False)\n","\n","agent = Agent(env, train_mode = True)\n"],"metadata":{"id":"8usx4HiayXZr","executionInfo":{"status":"ok","timestamp":1681050426513,"user_tz":-540,"elapsed":14,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["history = run_qlearning(agent, env, 1000)"],"metadata":{"id":"zSNRMhQ6yULS","executionInfo":{"status":"ok","timestamp":1681050426513,"user_tz":-540,"elapsed":14,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["Q_table = pd.DataFrame(agent.q_table)\n","\n","display(Q_table)"],"metadata":{"id":"cm8xmRPZycbD","colab":{"base_uri":"https://localhost:8080/","height":545},"executionInfo":{"status":"ok","timestamp":1681050426514,"user_tz":-540,"elapsed":14,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"7b4a7b58-fe91-4e27-8d2f-208535de6d62"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["            0          1          2          3\n","0  -12.496421 -10.396421   1.600027 -12.496421\n","1  -11.118125  -9.100000   2.308351  -7.000000\n","2   -7.000000   3.330247   0.000000   0.000000\n","3    0.000000   0.000000   0.000000   0.000000\n","4  -12.496421 -10.396421  -9.100000  -7.000000\n","5    0.000000   0.000000   0.000000   0.000000\n","6   -7.000000   4.804530   0.000000   0.000000\n","7    0.000000   0.000000   0.000000   0.000000\n","8  -12.496421  -9.100000  -7.000000  -7.000000\n","9   -7.000000 -10.396421  -7.000000  -7.000000\n","10  -7.000000   6.931472   0.000000   0.000000\n","11   0.000000   0.000000   0.000000   0.000000\n","12   0.000000   0.000000   0.000000   0.000000\n","13  -9.100000  -7.000000  -7.000000  -7.000000\n","14  -7.000000  -7.000000  10.000000   0.000000\n","15   0.000000   0.000000   0.000000   0.000000"],"text/html":["\n","  <div id=\"df-06a98af2-8581-44d7-bd95-ab0b30fb1b3a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-12.496421</td>\n","      <td>-10.396421</td>\n","      <td>1.600027</td>\n","      <td>-12.496421</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-11.118125</td>\n","      <td>-9.100000</td>\n","      <td>2.308351</td>\n","      <td>-7.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-7.000000</td>\n","      <td>3.330247</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-12.496421</td>\n","      <td>-10.396421</td>\n","      <td>-9.100000</td>\n","      <td>-7.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>-7.000000</td>\n","      <td>4.804530</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>-12.496421</td>\n","      <td>-9.100000</td>\n","      <td>-7.000000</td>\n","      <td>-7.000000</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>-7.000000</td>\n","      <td>-10.396421</td>\n","      <td>-7.000000</td>\n","      <td>-7.000000</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>-7.000000</td>\n","      <td>6.931472</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>-9.100000</td>\n","      <td>-7.000000</td>\n","      <td>-7.000000</td>\n","      <td>-7.000000</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>-7.000000</td>\n","      <td>-7.000000</td>\n","      <td>10.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06a98af2-8581-44d7-bd95-ab0b30fb1b3a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-06a98af2-8581-44d7-bd95-ab0b30fb1b3a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-06a98af2-8581-44d7-bd95-ab0b30fb1b3a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["#history"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c-GX9grtVpEo","executionInfo":{"status":"ok","timestamp":1681050426515,"user_tz":-540,"elapsed":12,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"c3cd6885-fb14-4b46-cf06-489acdedc803"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["\"\"\"\n","plt.figure(figsize = (20, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(history[\"n_moves\"][:, 0])\n","plt.subplot(1, 2, 2,)\n","plt.plot(history[\"rewards\"][:, 1])\n","plt.show()\n","\"\"\";"],"metadata":{"id":"Ldnp1nqV_zOM","executionInfo":{"status":"ok","timestamp":1681050426917,"user_tz":-540,"elapsed":409,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["env = gym.make('FrozenLake-v1', \n","               desc = None, \n","               map_name = \"4x4\", \n","               is_slippery = False)\n","\n","state = env.reset(seed = 123)\n","\n","print(state)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UCOPOp-oAZcO","executionInfo":{"status":"ok","timestamp":1681050426917,"user_tz":-540,"elapsed":8,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"0996e13c-8db3-409b-c84c-b6fd918b2c42"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["agent.set_train_mode(False)"],"metadata":{"id":"BV7MIDG5PGtY","executionInfo":{"status":"ok","timestamp":1681050426917,"user_tz":-540,"elapsed":7,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["done = False\n","\n","while done == False:\n","    action = agent.choose_action(state)\n","    next_s, reward, done, _ = env.step(action)\n","\n","    print(\"State: {}, action: {}, next_s: {}, reward: {}, done: {}.\".format(state, action, next_s, reward, done))\n","\n","    state = next_s"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K83nH25MNZbs","executionInfo":{"status":"ok","timestamp":1681050426918,"user_tz":-540,"elapsed":7,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"69b0e4f2-91be-46c2-c6a7-4105ea298cd7"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["State: 0, action: 2, next_s: 1, reward: 0.0, done: False.\n","State: 1, action: 2, next_s: 2, reward: 0.0, done: False.\n","State: 2, action: 1, next_s: 6, reward: 0.0, done: False.\n","State: 6, action: 1, next_s: 10, reward: 0.0, done: False.\n","State: 10, action: 1, next_s: 14, reward: 0.0, done: False.\n","State: 14, action: 2, next_s: 15, reward: 1.0, done: True.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"IRWWETQ-PjTI","executionInfo":{"status":"ok","timestamp":1681050426918,"user_tz":-540,"elapsed":3,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":16,"outputs":[]}]}