{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPA/Q6OgcfbFH5b68PuV/zX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# KerasCV Object Detection Training\n","https://keras.io/guides/keras_cv/object_detection_keras_cv/\n","\n","https://www.tensorflow.org/datasets/catalog/voc"],"metadata":{"id":"ElFVoaomCR1z"}},{"cell_type":"code","source":["try:\n","    import keras_core as keras\n","except:\n","    !pip install keras_core\n","    import keras_core as keras\n","\n","try:\n","    import keras_cv\n","except:\n","    !pip -q install keras_cv\n","    import keras_cv\n","\n","import os\n","import resource\n","import tqdm\n","\n","import numpy as np\n","\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tensorflow import keras\n","from tensorflow.keras import optimizers\n","\n","from keras_cv import bounding_box\n","from keras_cv import visualization\n","\n","import tensorflow as tf\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","print(tf.config.list_physical_devices('GPU'))\n","\n","if len(tf.config.list_physical_devices('GPU')) > 0:\n","    device_name = '/GPU:0'\n","else:\n","    device_name = \"/CPU:0\""],"metadata":{"id":"HfxsOo0mCR7S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689691467305,"user_tz":-540,"elapsed":11183,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"2e2fcf9d-d1b0-481b-ee97-65baddeda809"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Using TensorFlow backend\n","Num GPUs Available:  1\n","[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}]},{"cell_type":"markdown","source":["# Transfer Learning"],"metadata":{"id":"AfLIzjW3EHdH"}},{"cell_type":"code","source":["# Dataloaders.\n","\n","def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n","    inputs = next(iter(inputs.take(1)))\n","    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n","    visualization.plot_bounding_box_gallery(\n","        images,\n","        value_range=value_range,\n","        rows=rows,\n","        cols=cols,\n","        y_true=bounding_boxes,\n","        scale=5,\n","        font_scale=0.7,\n","        bounding_box_format=bounding_box_format,\n","        class_mapping=class_mapping,\n","    )\n","\n","\n","def unpackage_raw_tfds_inputs(inputs, bounding_box_format):\n","    image = inputs[\"image\"]\n","    boxes = keras_cv.bounding_box.convert_format(\n","        inputs[\"objects\"][\"bbox\"],\n","        images=image,\n","        source=\"rel_yxyx\",\n","        target=bounding_box_format,\n","    )\n","    bounding_boxes = {\n","        \"classes\": tf.cast(inputs[\"objects\"][\"label\"], dtype=tf.float32),\n","        \"boxes\": tf.cast(boxes, dtype=tf.float32),\n","    }\n","    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n","\n","\n","def load_pascal_voc(split, dataset, bounding_box_format):\n","    # https://www.tensorflow.org/datasets/catalog/voc\n","    ds = tfds.load(dataset, split=split, with_info=False, shuffle_files=True)\n","    ds = ds.map(\n","        lambda x: unpackage_raw_tfds_inputs(x, bounding_box_format=bounding_box_format),\n","        num_parallel_calls=tf.data.AUTOTUNE,\n","    )\n","    return ds"],"metadata":{"id":"Q-bx948OEJfV","executionInfo":{"status":"ok","timestamp":1689691467305,"user_tz":-540,"elapsed":2,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Pascal VOC classes.\n","\n","class_ids = [\n","    \"Aeroplane\",\n","    \"Bicycle\",\n","    \"Bird\",\n","    \"Boat\",\n","    \"Bottle\",\n","    \"Bus\",\n","    \"Car\",\n","    \"Cat\",\n","    \"Chair\",\n","    \"Cow\",\n","    \"Dining Table\",\n","    \"Dog\",\n","    \"Horse\",\n","    \"Motorbike\",\n","    \"Person\",\n","    \"Potted Plant\",\n","    \"Sheep\",\n","    \"Sofa\",\n","    \"Train\",\n","    \"Tvmonitor\",\n","    \"Total\",\n","]\n","class_mapping = dict(zip(range(len(class_ids)), class_ids))"],"metadata":{"id":"YTrW6s4K8LrT","executionInfo":{"status":"ok","timestamp":1689691467306,"user_tz":-540,"elapsed":3,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 4\n","\n","train_ds = load_pascal_voc(\n","    split=\"train\", dataset=\"voc/2007\", bounding_box_format=\"xywh\"\n",")\n","\n","eval_ds = load_pascal_voc(\n","    split=\"validation\", dataset=\"voc/2007\", bounding_box_format=\"xywh\"\n",")\n","\n","train_ds = train_ds.shuffle(BATCH_SIZE * 4)\n","\n","train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n","eval_ds = eval_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)"],"metadata":{"id":"b4IHhpl08IJp","executionInfo":{"status":"ok","timestamp":1689691472728,"user_tz":-540,"elapsed":5424,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["print(\"Train dataset length: {}.\".format(len(train_ds)))\n","print(\"Evaluation dataset length: {}\".format(len(eval_ds)))"],"metadata":{"id":"5SUZx1Wk3tMt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689691472729,"user_tz":-540,"elapsed":16,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"27898e94-9bec-4d6b-ac1b-2e1cdc7d37ff"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataset length: 625.\n","Evaluation dataset length: 627\n"]}]},{"cell_type":"code","source":["\"\"\"\n","visualize_dataset(\n","    train_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2\n",")\n","\"\"\";"],"metadata":{"id":"wrN4eKvYEON2","executionInfo":{"status":"ok","timestamp":1689691472729,"user_tz":-540,"elapsed":12,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","visualize_dataset(\n","    eval_ds,\n","    bounding_box_format=\"xywh\",\n","    value_range=(0, 255),\n","    rows=2,\n","    cols=2,\n","    # If you are not running your experiment on a local machine, you can also\n","    # make `visualize_dataset()` dump the plot to a file using `path`:\n","    # path=\"eval.png\"\n",")\n","\"\"\";"],"metadata":{"id":"Cjj7-x5jEShM","executionInfo":{"status":"ok","timestamp":1689691472729,"user_tz":-540,"elapsed":11,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Image augmentation for training.\n","\n","augmenter = keras.Sequential(\n","    layers=[\n","        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xywh\"),\n","        keras_cv.layers.JitteredResize(\n","            target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xywh\"\n","        ),\n","    ]\n",")\n","\n","train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","\"\"\"\n","visualize_dataset(\n","    train_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2\n",")\n","\"\"\";"],"metadata":{"id":"AwrftOZlEUnU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689691482783,"user_tz":-540,"elapsed":10065,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}},"outputId":"e04101ff-f49d-424f-e600-3c53bb5534c4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'images': tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"RaggedFromVariant_2/RaggedTensorFromVariant:2\", shape=(None, 3), dtype=float32), row_splits=Tensor(\"RaggedFromVariant_2/RaggedTensorFromVariant:1\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant_2/RaggedTensorFromVariant:0\", shape=(5,), dtype=int64)), 'bounding_boxes': {'classes': tf.RaggedTensor(values=Tensor(\"RaggedFromVariant_1/RaggedTensorFromVariant:1\", shape=(None,), dtype=float32), row_splits=Tensor(\"RaggedFromVariant_1/RaggedTensorFromVariant:0\", shape=(5,), dtype=int64)), 'boxes': tf.RaggedTensor(values=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:1\", shape=(None, 4), dtype=float32), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(5,), dtype=int64))}}. Consider rewriting this model with the Functional API.\n"]}]},{"cell_type":"code","source":["inference_resizing = keras_cv.layers.Resizing(\n","    640, 640, bounding_box_format=\"xywh\", pad_to_aspect_ratio=True\n",")\n","\n","eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","\"\"\"\n","visualize_dataset(\n","    eval_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2\n",")\n","\"\"\";"],"metadata":{"id":"FAT1WNU7EaMq","executionInfo":{"status":"ok","timestamp":1689691484024,"user_tz":-540,"elapsed":1248,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Unpackage inputs from preprocessing to feed into the model.\n","\n","def dict_to_tuple(inputs):\n","    return inputs[\"images\"], bounding_box.to_dense(\n","        inputs[\"bounding_boxes\"], max_boxes=32\n","    )\n","\n","with tf.device(device_name):\n","    train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n","    eval_ds = eval_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n","    eval_ds = eval_ds.prefetch(tf.data.AUTOTUNE)"],"metadata":{"id":"9oU8Iq2uEevM","executionInfo":{"status":"ok","timestamp":1689691484025,"user_tz":-540,"elapsed":13,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Metrics.\n","\n","coco_metrics = keras_cv.metrics.BoxCOCOMetrics(\n","    bounding_box_format=\"xywh\", evaluate_freq=20\n",")"],"metadata":{"id":"_833EIILHSfL","executionInfo":{"status":"ok","timestamp":1689691484025,"user_tz":-540,"elapsed":10,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def print_metrics(metrics, result):\n","    maxlen = max([len(key) for key in result.keys()])\n","    print(\"Metrics:\")\n","    print(\"-\" * (maxlen + 1))\n","    for k, v in metrics.items():\n","        print(f\"{k.ljust(maxlen+1)}: {v.numpy():0.2f}\")"],"metadata":{"id":"P0EzofmmHUbA","executionInfo":{"status":"ok","timestamp":1689691484026,"user_tz":-540,"elapsed":10,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["#pretrained_model.compile(\n","#    classification_loss=\"focal\",\n","#    box_loss=\"smoothl1\",\n","#    optimizer=optimizer,\n","#    metrics=[coco_metrics],\n","#)\n","#coco_metrics.reset_state()\n","#result = pretrained_model.evaluate(eval_ds.take(40), verbose=0)\n","#result = coco_metrics.result(force=True)\n","\n","#print_metrics(result)"],"metadata":{"id":"FpVF-1GlHWLW","executionInfo":{"status":"ok","timestamp":1689691484026,"user_tz":-540,"elapsed":9,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n","    def __init__(self, data):\n","        super().__init__()\n","        self.data = data\n","        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n","            bounding_box_format=\"xywh\",\n","            # passing 1e9 ensures we never evaluate until\n","            # `metrics.result(force=True)` is\n","            # called.\n","            evaluate_freq=1e9,\n","        )\n","\n","    def on_epoch_end(self, epoch, logs):\n","        self.metrics.reset_state()\n","        for batch in tqdm.tqdm(self.data):\n","            images, y_true = batch[0], batch[1]\n","            y_pred = self.model.predict(images, verbose=0)\n","            self.metrics.update_state(y_true, y_pred)\n","\n","        metrics = self.metrics.result(force=True)\n","        logs.update(metrics)\n","        return logs"],"metadata":{"id":"OT0SW99tHZ58","executionInfo":{"status":"ok","timestamp":1689691484026,"user_tz":-540,"elapsed":9,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Prepare optimizer.\n","\n","base_lr = 0.005\n","\n","# Including a global_clipnorm is extremely important in object detection tasks\n","optimizer = tf.keras.optimizers.SGD(\n","    learning_rate=base_lr, momentum=0.9, global_clipnorm=10.0\n",")"],"metadata":{"id":"hbkF2dtTYFhx","executionInfo":{"status":"ok","timestamp":1689691484026,"user_tz":-540,"elapsed":8,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# https://keras.io/api/keras_cv/models/\n","\n","model = \"yolo\"\n","\n","#preset = \"resnet50_imagenet\"\n","preset = \"yolo_v8_xs_backbone_coco\"\n","\n","with tf.device(device_name):\n","    if model == \"retinanet\":\n","        model = keras_cv.models.RetinaNet.from_preset(\n","            preset,\n","            num_classes=len(class_mapping),\n","            bounding_box_format=\"xywh\",\n","        )\n","\n","        classification_loss = \"focal\"\n","        box_loss = \"smoothl1\"\n","\n","    elif model == \"yolo\":\n","        model = keras_cv.models.YOLOV8Detector.from_preset(\n","            preset,\n","            num_classes=len(class_mapping),\n","            bounding_box_format=\"xywh\",\n","        )\n","\n","        classification_loss = 'binary_crossentropy'\n","        box_loss = 'ciou'\n","\n","    # Compile model on device.\n","    model.compile(\n","        classification_loss=classification_loss,\n","        box_loss=box_loss,\n","        optimizer=optimizer,\n","        jit_compile=False,\n","    )"],"metadata":{"id":"dRiQk_S3HaMj","executionInfo":{"status":"ok","timestamp":1689691487028,"user_tz":-540,"elapsed":3010,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["#print(model.backbone.summary())"],"metadata":{"id":"4FFf_O2RZaUT","executionInfo":{"status":"ok","timestamp":1689691487028,"user_tz":-540,"elapsed":6,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["epochs = 10\n","\n","with tf.device(device_name):\n","    history = model.fit(\n","        train_ds,\n","        validation_data=eval_ds,\n","        epochs=epochs,\n","        callbacks=[EvaluateCOCOMetricsCallback(eval_ds)],\n","    )"],"metadata":{"id":"zpKlv2JtHh1M","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7b019fbf-c212-4532-b9fa-fed9da635a6b","executionInfo":{"status":"ok","timestamp":1689691890267,"user_tz":-540,"elapsed":403243,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["625/625 [==============================] - ETA: 0s - loss: 24.8552 - box_loss: 3.1573 - class_loss: 21.6979"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 627/627 [03:13<00:00,  3.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r625/625 [==============================] - 371s 541ms/step - loss: 24.8552 - box_loss: 3.1573 - class_loss: 21.6979 - val_loss: 3.6553 - val_box_loss: 3.3764 - val_class_loss: 0.2789 - MaP: 0.0000e+00 - MaP@[IoU=50]: 0.0000e+00 - MaP@[IoU=75]: 0.0000e+00 - MaP@[area=small]: 0.0000e+00 - MaP@[area=medium]: 0.0000e+00 - MaP@[area=large]: 0.0000e+00 - Recall@[max_detections=1]: 0.0000e+00 - Recall@[max_detections=10]: 0.0000e+00 - Recall@[max_detections=100]: 0.0000e+00 - Recall@[area=small]: 0.0000e+00 - Recall@[area=medium]: 0.0000e+00 - Recall@[area=large]: 0.0000e+00\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"8Uh_ir-LXqNO","executionInfo":{"status":"ok","timestamp":1689691890269,"user_tz":-540,"elapsed":20,"user":{"displayName":"Yuki Natsume","userId":"12150857735362412716"}}},"execution_count":18,"outputs":[]}]}